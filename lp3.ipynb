{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation packages\n",
    "import pandas as pd  # Pandas provides data structures for efficiently storing large datasets and tools for data analysis.\n",
    "import numpy as np   # NumPy is used for numerical operations and array manipulations.\n",
    "\n",
    "# Data Visualization packages\n",
    "import matplotlib.pyplot as plt  # Matplotlib is a 2D plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "import seaborn as sns  # Seaborn is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "# Machine learning Packages\n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.compose import ColumnTransformer  # ColumnTransformer allows applying different transformers to different columns in a dataset.\n",
    "from sklearn.impute import SimpleImputer  # SimpleImputer is used for handling missing data by imputing missing values with specified strategies.\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  # These classes provide different methods for scaling/normalizing numerical features.\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder  # These classes handle categorical feature encoding (one-hot, label, and ordinal encoding).\n",
    "from sklearn.preprocessing import FunctionTransformer  # FunctionTransformer allows applying custom functions to transform data.\n",
    "from sklearn.tree import DecisionTreeClassifier  # DecisionTreeClassifier is an implementation of a decision tree classifier.\n",
    "from sklearn.metrics import accuracy_score, classification_report  # These metrics are used for evaluating classification model performance.\n",
    "from sklearn.model_selection import train_test_split  # train_test_split is used to split a dataset into training and testing sets.\n",
    "from sklearn import set_config  # set_config allows configuring global scikit-learn behavior.\n",
    "from sklearn.ensemble import RandomForestClassifier  # RandomForestClassifier is an ensemble learning method based on decision trees.\n",
    "from sklearn.svm import SVC  # Support Vector Classifier (SVC) is a classifier that uses support vector machines for classification.\n",
    "from sklearn.preprocessing import PowerTransformer  # PowerTransformer applies power transformations to make data more Gaussian-like.\n",
    "from sklearn.naive_bayes import GaussianNB  # Gaussian Naive Bayes is a probabilistic classifier based on the Gaussian distribution.\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # GradientBoostingClassifier is an ensemble method that builds a sequence of weak learners (trees).\n",
    "from scipy.stats import pearsonr  # Pearson correlation coefficient measures the linear relationship between two variables.\n",
    "from sklearn.model_selection import cross_val_score  # cross_val_score is used for cross-validated model performance evaluation.\n",
    "from imblearn.under_sampling import RandomUnderSampler  # RandomUnderSampler is used for under-sampling to address class imbalance.\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE  # RandomOverSampler and SMOTE are used for over-sampling to address class imbalance.\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif  # SelectKBest performs feature selection based on scoring functions like mutual information.\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE is a technique for generating synthetic samples to address class imbalance.\n",
    "from sklearn.datasets import make_classification  # make_classification generates a synthetic dataset for classification.\n",
    "from sklearn.metrics import roc_curve, roc_auc_score  # roc_curve and roc_auc_score are used for Receiver Operating Characteristic (ROC) curve analysis.\n",
    "from sklearn.metrics import confusion_matrix  # confusion_matrix calculates the confusion matrix for classification models.\n",
    "from sklearn.model_selection import GridSearchCV  # GridSearchCV performs hyperparameter tuning using grid search.\n",
    "\n",
    "# Database connection package\n",
    "import pyodbc  # PyODBC is a Python module that makes accessing ODBC databases simple.\n",
    "from dotenv import dotenv_values  # dotenv loads environment variables from a .env file.\n",
    "\n",
    "# Ignore warnings (optional)\n",
    "import warnings  # The warnings module provides a way to handle warnings in Python.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"server_name\")\n",
    "database = environment_variables.get(\"database_name\")\n",
    "username = environment_variables.get(\"username\")\n",
    "password = environment_variables.get(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};MARS_Connection=yes;MinP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "# This will connect to the server \n",
    "\n",
    "\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the sql query to get the data is what what you see below. \n",
    "# Note that you will not have permissions to insert delete or update this database table. \n",
    "\n",
    "query = \"SELECT * FROM dbo.oil\"\n",
    "\n",
    "oil_table = pd.read_sql(query, connection)\n",
    "\n",
    "#Holiday_events table \n",
    "\n",
    "query = \"SELECT * FROM dbo.holidays_events\"\n",
    "\n",
    "holiday_events = pd.read_sql(query, connection)\n",
    "\n",
    "#Stores Table\n",
    "\n",
    "query = \"SELECT * FROM dbo.stores\"\n",
    "\n",
    "stores_table = pd.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the third dataset (it's a CSV file named 'test', 'transaction', 'sample_submission')\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "transaction_data = pd.read_csv('transactions.csv')\n",
    "\n",
    "sample_data = pd.read_csv('sample_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
